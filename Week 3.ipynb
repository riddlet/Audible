{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week's task is to try to run some taggers on the Audible data given to us by Dan. We have two sets of files. The first is an unlabeled dataset that contains approximately 25% of the text from 1000 books. The second dataset consists of 10 books from the first dataset in XML files that are labeled with parts of speech, word chunks (i.e. verb phrases, noun phrases), lemmatized, and the basic entities (People, Organizations, and Locations). Let's do some exploration of these files.\n",
    "\n",
    "First, let's find out how many individual words are given entity tags, and what type of tags they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lxml.etree as etree\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "from __future__ import division\n",
    "\n",
    "files = glob.glob('data/audible/processedText/*.xml')\n",
    "entities = {'P': 0, 'L':0, 'O':0, 'D':0}\n",
    "total_tokens = 0\n",
    "for f in files:\n",
    "    xml_data = open(f).read()\n",
    "    soup = BeautifulSoup(xml_data, 'lxml')\n",
    "    for i in soup.find_all(['w', 'c']):\n",
    "        for item in i:\n",
    "            total_tokens += 1\n",
    "            if 'ner' in i.attrs:\n",
    "                entities[i.attrs['ner'][0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9057 instances of P\n",
      "There are 1407 instances of L\n",
      "There are 391 instances of O\n",
      "There are 0 instances of D\n",
      "***** 326316 tokens ******\n"
     ]
    }
   ],
   "source": [
    "for i in entities.keys():\n",
    "    print('There are ' + str(entities[i]) + ' instances of ' + str(i))\n",
    "print('***** ' + str(total_tokens) + ' tokens ******')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the labeled data, we see that there are just over 326k individual tokens, and nearly 11k of them are labeled as entities. The majority of these entities (~9k) are people, with a few location tags, and a small number of Organizations. Dates are not labeled in these data, so we'll exclude them from our experimentation.\n",
    "\n",
    "One thing to note is that this method of counting double-counts some instances. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = glob.glob('data/audible/processedText/*.xml')\n",
    "total_tokens = 0\n",
    "unique_entities = {}\n",
    "for f in files:\n",
    "    xml_data = open(f).read()\n",
    "    soup = BeautifulSoup(xml_data, 'lxml')\n",
    "    for i in soup.find_all(lambda tag: 'ner' in tag.attrs):\n",
    "        if i['ner']+'_'+f[-14:-4] in unique_entities.keys():\n",
    "            unique_entities[i['ner']+'_'+f[-14:-4]] += 1\n",
    "        else:\n",
    "            unique_entities[i['ner']+'_'+f[-14:-4]] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entity that takes up the largest number of strings is:\n",
      "Harmonic Field of Glass Bells and Green Gig\n",
      "\n",
      "\n",
      "*****************\n",
      "The entire sentence that contains this entity is:\n",
      "We might add here that later on the constructors had an article published in a prominent scientific journal under the title of ' Recursive β – Metafunctions in the Special Case of a Bogus Polypolice Transmogrification Conversion on an Oscillating Harmonic Field of Glass Bells and Green Gig , Kerosene Lamp on the Left to Divert Attention , Solved by Beastly Incarceration – Concatenation ' , which was subsequently exploited by the tabloids as ' The Police State Rears Its Ugly Head ' .\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "longest_entity = max(unique_entities.iteritems(), key=operator.itemgetter(1))\n",
    "ent_id, file_id = longest_entity[0].split('_')\n",
    "xml_data = open('data/audible/processedText/'+file_id+'.xml').read()\n",
    "soup = BeautifulSoup(xml_data, 'lxml')\n",
    "entity = soup.findAll(ner=ent_id)\n",
    "print('The entity that takes up the largest number of strings is:')\n",
    "print(' '.join(i.text for i in entity))\n",
    "print('\\n')\n",
    "print('*****************')\n",
    "print('The entire sentence that contains this entity is:')\n",
    "print(' '.join(entity[0].parent.parent.text.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the former counting method, this entity would have been counted as 8 distinct entities, whereas it's actually meant to be one entity. Interestingly, this example is incorrectly labeled. This entity is labeled as an organization, but it's actually part of a title (and would make a great band name!). Regardless, this highlights the importance of why using a standard scoring approach wont work so well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Entities\n",
    "\n",
    "Below, I explore how Spacy handles entities in this text. The standard way of scoring this type of data is the one used for the CONLL shared tasks. The perl CONLL scoring script is available [here](http://www.cnts.ua.ac.be/conll2002/ner/bin/conlleval.txt), and I use this as my scorer. One of the difficulties here is that the spacy tokenizer works a bit differently than the tokenizer used on the Audible data. Below, I print out a sample of one of the documents with the ner tags. The printout is in the following format:\n",
    "\n",
    "\n",
    "|spacy_text   |spacy_tag   |******   |audible_text   |audible_tag   |\n",
    "|---|---|--:|---|---|\n",
    "|disemboweled   |O-   |******   |disemboweled   |O   |\n",
    "|,   |O-   |******   |,   |O   |\n",
    "|buried   |O-   |******   |buried   |O   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disemboweled\tO-\t******  disemboweled\tO\n",
      ",\tO-\t******  ,\tO\n",
      "buried\tO-\t******  buried\tO\n",
      "alive\tO-\t******  alive\tO\n",
      ",\tO-\t******  ,\tO\n",
      "crucified\tO-\t******  crucified\tO\n",
      "and\tO-\t******  and\tO\n",
      "burnt\tO-\t******  burnt\tO\n",
      "at\tO-\t******  at\tO\n",
      "the\tO-\t******  the\tO\n",
      "stake\tO-\t******  stake\tO\n",
      ",\tO-\t******  ,\tO\n",
      "after\tO-\t******  after\tO\n",
      "which\tO-\t******  which\tO\n",
      "your\tO-\t******  your\tO\n",
      "ashes\tO-\t******  ashes\tO\n",
      "shall\tO-\t******  shall\tO\n",
      "be\tO-\t******  be\tO\n",
      "sent\tO-\t******  sent\tO\n",
      "into\tO-\t******  into\tO\n",
      "orbit\tO-\t******  orbit\tO\n",
      "as\tO-\t******  as\tO\n",
      "a\tO-\t******  a\tO\n",
      "warning\tO-\t******  warning\tO\n",
      "and\tO-\t******  and\tO\n",
      "perpetual\tO-\t******  perpetual\tO\n",
      "reminder\tO-\t******  reminder\tO\n",
      "to\tO-\t******  to\tO\n",
      "all\tO-\t******  all\tO\n",
      "would\tO-\t******  would-be\tO\n",
      "-\tO-\t******  regicides\tO\n",
      "be\tO-\t******  ,\tO\n",
      "regicides\tO-\t******  amen\tO\n",
      ",\tO-\t******  .\tO\n",
      "amen\tO-\t******  '\tO\n",
      ".\tO-\t******  '\tO\n",
      "'\tO-\t******  Ca\tO\n",
      "'\tO-\t******  n't\tO\n",
      "Ca\tO-\t******  you\tO\n",
      "n't\tO-\t******  wait\tO\n",
      "you\tO-\t******  a\tO\n",
      "wait\tO-\t******  bit\tO\n",
      "a\tO-\t******  ?\tO\n",
      "bit\tO-\t******  '\tO\n",
      "?\tO-\t******  asked\tO\n"
     ]
    }
   ],
   "source": [
    "words = soup.find_all(['w', 'c'])\n",
    "doc = nlp(' '.join(w.text for w in words))\n",
    "start = 175\n",
    "subdoc = doc[start:220]\n",
    "for i, w in enumerate(subdoc):\n",
    "    if 'ner' in words[i+start].attrs:\n",
    "        print w.text + '\\t' + w.ent_iob_ + '-' + w.ent_type_ + '\\t******  ' + words[i+start].text + '\\t' + words[i+start].attrs['ner']\n",
    "    else:\n",
    "        print w.text + '\\t' + w.ent_iob_ + '-' + w.ent_type_ + '\\t******  ' + words[i+start].text + '\\t' + 'O'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that the compound word would-be (about halfway down) is treated differently by the two systems. This makes lining up the annotations a bit of a challenge. Especially since it may mean that Spacy will provide multiple tags to the same word which is given only one tag from Audible. To counteract this, I'll group together the words that are split apart, and give them an entity tag if any of the tokens contains one. Fortunately, there were no cases where these split words contained two different tags.\n",
    "\n",
    "I've collected some utility functions (e.g. the one that condenses tags as described above) and placed them in a separate file, `audible.py` found [here](https://github.com/riddlet/Audible/blob/master/audible.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ugh, this is ugly.\n",
    "from audible import condense_tokens\n",
    "\n",
    "spacy_text = []\n",
    "spacy_tag = []\n",
    "audible_text = []\n",
    "audible_tag = []\n",
    "for f in files:\n",
    "    xml_data = open(f).read()\n",
    "    soup = BeautifulSoup(xml_data, 'lxml')\n",
    "    words = soup.find_all(['w', 'c'])\n",
    "    doc = nlp(' '.join(w.text for w in words))\n",
    "    offset = [0, 0] #spacy, audible\n",
    "    for i, w in enumerate(doc):\n",
    "        if i+offset[1] == len(words):\n",
    "            break\n",
    "        spacy_tokens = nlp(words[i+offset[1]].text) #check to see if the tokenizations match\n",
    "\n",
    "        if len(spacy_tokens)==1: #if they match\n",
    "            if 'ner' in words[i+offset[1]].attrs:\n",
    "                spacy_text.append(doc[i+offset[0]].text)\n",
    "                spacy_tag.append(doc[i+offset[0]].ent_iob_ + '-' + doc[i+offset[0]].ent_type_)\n",
    "                audible_text.append(words[i+offset[1]].text)\n",
    "                audible_tag.append(words[i+offset[1]].attrs['ner'])\n",
    "            else:\n",
    "                spacy_text.append(doc[i+offset[0]].text)\n",
    "                spacy_tag.append(doc[i+offset[0]].ent_iob_ + '-' + doc[i+offset[0]].ent_type_)\n",
    "                audible_text.append(words[i+offset[1]].text)\n",
    "                audible_tag.append('O')\n",
    "        if len(spacy_tokens)>1: #if spacy splits it up into more tokens\n",
    "            stext, stag, atext, atag, offset = condense_tokens(doc, words, i, offset, nlp)\n",
    "            spacy_text.append(stext)\n",
    "            spacy_tag.append(stag)\n",
    "            audible_text.append(atext)\n",
    "            audible_tag.append(atag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'spacy_text':spacy_text,\n",
    "             'spacy_tag':spacy_tag,\n",
    "             'audible_text':audible_text,\n",
    "             'audible_tag':audible_tag})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing Tags\n",
    "Spacy's entity recognizer includes many other categories that are not listed in the Audible data. The next bits of code removes those entities from the tags provided by Spacy and then makes the remaining ones in a standard format that is similar to that expected by the CONLL script. I then standardize the tags from the Audible data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O-     315119\n",
       "B-P      7195\n",
       "B-L      1341\n",
       "I-P       977\n",
       "B-O       904\n",
       "I-O       592\n",
       "I-L       188\n",
       "Name: cleaned_spacy_tags, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from audible import clean_spacy_tags\n",
    "\n",
    "df['cleaned_spacy_tags'] = df.spacy_tag.apply(clean_spacy_tags)\n",
    "\n",
    "df.cleaned_spacy_tags.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above provides some initial indication that Spacy is a reasonable approximation to the tags in the Audible data. We can tell this because People tags make up a larger share of the tags than do Locations or Organizations. Though we also see that there appear to be a larger number of organizations (904) here than in the audible data (391).\n",
    "\n",
    "I now convert the audible tags into the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "O-     315461\n",
       "I-P      5857\n",
       "B-P      3200\n",
       "B-L       743\n",
       "I-L       664\n",
       "I-O       235\n",
       "B-O       156\n",
       "Name: aud_clean, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from audible import clean_audible_tags\n",
    "\n",
    "df['aud_clean'] = df.audible_tag.apply(clean_audible_tags)\n",
    "df['counter'] = df.groupby('audible_tag').cumcount()\n",
    "\n",
    "df['iob_sys'] = 'O'\n",
    "\n",
    "df['iob_sys'][df.counter==0] = 'B'\n",
    "df['iob_sys'][df.counter>0] = 'I'\n",
    "df['aud_clean'][df.aud_clean!='O-'] = df['iob_sys']+df['aud_clean']\n",
    "df.aud_clean.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy Scoring\n",
    "Below, I write a file for the conll script. I think that evaluation here consists of entire chunks, rather than individualized tokens. As such, if System one identified \"Susan Brown\" as one entity chunk, and system two only identified \"Susan\" as the chunk, then this would be incorrect.\n",
    "\n",
    "The performance below is treating the audible data as the 'gold standard'. We can see that there's generally good agreement, though it seems as though the audible and spacy systems frequently disagree on organizational tags, and Location tags have pretty middling performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "outdat = df[['audible_text', 'aud_clean', 'cleaned_spacy_tags']]\n",
    "outdat['audible_text'] = outdat.audible_text.apply(lambda x: x.replace(' ', ''))\n",
    "outdat.to_csv('temp', sep=' ', encoding='utf-8', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processed 326316 tokens with 9108 phrases; found: 9488 phrases; correct: 6682.\n",
    "\n",
    "`accuracy:  97.33%; precision:  70.43%; recall:  73.36%; FB1:  71.86`\n",
    "\n",
    "                L: precision:  48.88%; recall:  62.96%; FB1:  55.03  1342\n",
    "                \n",
    "                O: precision:   5.53%; recall:  25.00%; FB1:   9.06  940\n",
    "                \n",
    "                P: precision:  82.90%; recall:  76.02%; FB1:  79.31  7206"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that performance is a reasonable approximation to the Audible tags. Identifying Persons is most consistent, with Locations producing more middling results, and Organizations matching rather poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoreNLP system\n",
    "\n",
    "Dan's initial impression was that the Kindle system audible uses to tag their books is based on Stanford's CoreNLP system, or at least was very similar. Here, I'm using the NLTK wrapper to stanford's tagger. As in the spacy case, I've also built in some tools to handle instances in which the tokenization between the two systems does not match. This may result in some biases to the evaluation metrics. I haven't done a systematic exploration of the direction of this bias, but I can't think of a reason why this would bias the evaluation upwards. And I suspect that any bias introduced is small (less than 1%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import NERTagger\n",
    "st = NERTagger('../stanford-ner-2017-06-09/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "               '../stanford-ner-2017-06-09/stanford-ner-3.8.0.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from audible import condense_stanford_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** The cell below takes a long time to run (~24 hours on my 2013-era MacBook Air). This could be reduced through parallelization, or by running the command line tool with many of the extra features turned off. I'm not sure whether this wrapper exposes arguments to switch off these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/audible/processedText/B000FCKA6C.xml\n",
      "0.0 of document complete\n",
      "0.0473260766682 of document complete\n",
      "0.0946521533365 of document complete\n",
      "0.141978230005 of document complete\n",
      "0.189304306673 of document complete\n",
      "0.236630383341 of document complete\n",
      "0.283956460009 of document complete\n",
      "0.331282536678 of document complete\n",
      "0.378608613346 of document complete\n",
      "0.425934690014 of document complete\n",
      "0.473260766682 of document complete\n",
      "0.520586843351 of document complete\n",
      "0.567912920019 of document complete\n",
      "0.615238996687 of document complete\n",
      "0.662565073355 of document complete\n",
      "0.709891150024 of document complete\n",
      "0.757217226692 of document complete\n",
      "0.80454330336 of document complete\n",
      "0.851869380028 of document complete\n",
      "0.899195456697 of document complete\n",
      "0.946521533365 of document complete\n",
      "0.993847610033 of document complete\n",
      "data/audible/processedText/B003ZSHUK2.xml\n",
      "0.0 of document complete\n",
      "0.043956043956 of document complete\n",
      "0.0879120879121 of document complete\n",
      "0.131868131868 of document complete\n",
      "0.175824175824 of document complete\n",
      "0.21978021978 of document complete\n",
      "0.263736263736 of document complete\n",
      "0.307692307692 of document complete\n",
      "0.351648351648 of document complete\n",
      "0.395604395604 of document complete\n",
      "0.43956043956 of document complete\n",
      "0.483516483516 of document complete\n",
      "0.527472527473 of document complete\n",
      "0.571428571429 of document complete\n",
      "0.615384615385 of document complete\n",
      "0.659340659341 of document complete\n",
      "0.703296703297 of document complete\n",
      "0.747252747253 of document complete\n",
      "0.791208791209 of document complete\n",
      "0.835164835165 of document complete\n",
      "0.879120879121 of document complete\n",
      "0.923076923077 of document complete\n",
      "0.967032967033 of document complete\n",
      "data/audible/processedText/B007C7369Q.xml\n",
      "0.0 of document complete\n",
      "0.0365898280278 of document complete\n",
      "0.0731796560556 of document complete\n",
      "0.109769484083 of document complete\n",
      "0.146359312111 of document complete\n",
      "0.182949140139 of document complete\n",
      "0.219538968167 of document complete\n",
      "0.256128796195 of document complete\n",
      "0.292718624222 of document complete\n",
      "0.32930845225 of document complete\n",
      "0.365898280278 of document complete\n",
      "0.402488108306 of document complete\n",
      "0.439077936334 of document complete\n",
      "0.475667764362 of document complete\n",
      "0.512257592389 of document complete\n",
      "0.548847420417 of document complete\n",
      "0.585437248445 of document complete\n",
      "0.622027076473 of document complete\n",
      "0.658616904501 of document complete\n",
      "0.695206732528 of document complete\n",
      "0.731796560556 of document complete\n",
      "0.768386388584 of document complete\n",
      "0.804976216612 of document complete\n",
      "0.84156604464 of document complete\n",
      "0.878155872667 of document complete\n",
      "0.914745700695 of document complete\n",
      "0.951335528723 of document complete\n",
      "0.987925356751 of document complete\n",
      "data/audible/processedText/B008VWZ3H4.xml\n",
      "0.0 of document complete\n",
      "0.0380517503805 of document complete\n",
      "0.076103500761 of document complete\n",
      "0.114155251142 of document complete\n",
      "0.152207001522 of document complete\n",
      "0.190258751903 of document complete\n",
      "0.228310502283 of document complete\n",
      "0.266362252664 of document complete\n",
      "0.304414003044 of document complete\n",
      "0.342465753425 of document complete\n",
      "0.380517503805 of document complete\n",
      "0.418569254186 of document complete\n",
      "0.456621004566 of document complete\n",
      "0.494672754947 of document complete\n",
      "0.532724505327 of document complete\n",
      "0.570776255708 of document complete\n",
      "0.608828006088 of document complete\n",
      "0.646879756469 of document complete\n",
      "0.684931506849 of document complete\n",
      "0.72298325723 of document complete\n",
      "0.76103500761 of document complete\n",
      "0.799086757991 of document complete\n",
      "0.837138508371 of document complete\n",
      "0.875190258752 of document complete\n",
      "0.913242009132 of document complete\n",
      "0.951293759513 of document complete\n",
      "0.989345509893 of document complete\n",
      "data/audible/processedText/B00A3KYSPU.xml\n",
      "0.0 of document complete\n",
      "0.041928721174 of document complete\n",
      "0.083857442348 of document complete\n",
      "0.125786163522 of document complete\n",
      "0.167714884696 of document complete\n",
      "0.20964360587 of document complete\n",
      "0.251572327044 of document complete\n",
      "0.293501048218 of document complete\n",
      "0.335429769392 of document complete\n",
      "0.377358490566 of document complete\n",
      "0.41928721174 of document complete\n",
      "0.461215932914 of document complete\n",
      "0.503144654088 of document complete\n",
      "0.545073375262 of document complete\n",
      "0.587002096436 of document complete\n",
      "0.62893081761 of document complete\n",
      "0.670859538784 of document complete\n",
      "0.712788259958 of document complete\n",
      "0.754716981132 of document complete\n",
      "0.796645702306 of document complete\n",
      "0.83857442348 of document complete\n",
      "0.880503144654 of document complete\n",
      "0.922431865828 of document complete\n",
      "0.964360587002 of document complete\n",
      "data/audible/processedText/B00HNIJ38C.xml\n",
      "0.0 of document complete\n",
      "0.0981354268891 of document complete\n",
      "0.196270853778 of document complete\n",
      "0.294406280667 of document complete\n",
      "0.392541707556 of document complete\n",
      "0.490677134446 of document complete\n",
      "0.588812561335 of document complete\n",
      "0.686947988224 of document complete\n",
      "0.785083415113 of document complete\n",
      "0.883218842002 of document complete\n",
      "0.981354268891 of document complete\n",
      "data/audible/processedText/B00K6Y0HIK.xml\n",
      "0.0 of document complete\n",
      "0.0926784059314 of document complete\n",
      "0.185356811863 of document complete\n",
      "0.278035217794 of document complete\n",
      "0.370713623726 of document complete\n",
      "0.463392029657 of document complete\n",
      "0.556070435589 of document complete\n",
      "0.64874884152 of document complete\n",
      "0.741427247451 of document complete\n",
      "0.834105653383 of document complete\n",
      "0.926784059314 of document complete\n",
      "data/audible/processedText/B00MNNAOJ4.xml\n",
      "0.0 of document complete\n",
      "0.0486618004866 of document complete\n",
      "0.0973236009732 of document complete\n",
      "0.14598540146 of document complete\n",
      "0.194647201946 of document complete\n",
      "0.243309002433 of document complete\n",
      "0.29197080292 of document complete\n",
      "0.340632603406 of document complete\n",
      "0.389294403893 of document complete\n",
      "0.43795620438 of document complete\n",
      "0.486618004866 of document complete\n",
      "0.535279805353 of document complete\n",
      "0.583941605839 of document complete\n",
      "0.632603406326 of document complete\n",
      "0.681265206813 of document complete\n",
      "0.729927007299 of document complete\n",
      "0.778588807786 of document complete\n",
      "0.827250608273 of document complete\n",
      "0.875912408759 of document complete\n",
      "0.924574209246 of document complete\n",
      "0.973236009732 of document complete\n",
      "data/audible/processedText/B00Z4M3EWU.xml\n",
      "0.0 of document complete\n",
      "0.0428449014567 of document complete\n",
      "0.0856898029135 of document complete\n",
      "0.12853470437 of document complete\n",
      "0.171379605827 of document complete\n",
      "0.214224507284 of document complete\n",
      "0.25706940874 of document complete\n",
      "0.299914310197 of document complete\n",
      "0.342759211654 of document complete\n",
      "0.385604113111 of document complete\n",
      "0.428449014567 of document complete\n",
      "0.471293916024 of document complete\n",
      "0.514138817481 of document complete\n",
      "0.556983718937 of document complete\n",
      "0.599828620394 of document complete\n",
      "0.642673521851 of document complete\n",
      "0.685518423308 of document complete\n",
      "0.728363324764 of document complete\n",
      "0.771208226221 of document complete\n",
      "0.814053127678 of document complete\n",
      "0.856898029135 of document complete\n",
      "0.899742930591 of document complete\n",
      "0.942587832048 of document complete\n",
      "0.985432733505 of document complete\n",
      "data/audible/processedText/B018FHCQVU.xml\n",
      "0.0 of document complete\n",
      "0.0392003136025 of document complete\n",
      "0.078400627205 of document complete\n",
      "0.117600940808 of document complete\n",
      "0.15680125441 of document complete\n",
      "0.196001568013 of document complete\n",
      "0.235201881615 of document complete\n",
      "0.274402195218 of document complete\n",
      "0.31360250882 of document complete\n",
      "0.352802822423 of document complete\n",
      "0.392003136025 of document complete\n",
      "0.431203449628 of document complete\n",
      "0.47040376323 of document complete\n",
      "0.509604076833 of document complete\n",
      "0.548804390435 of document complete\n",
      "0.588004704038 of document complete\n",
      "0.62720501764 of document complete\n",
      "0.666405331243 of document complete\n",
      "0.705605644845 of document complete\n",
      "0.744805958448 of document complete\n",
      "0.78400627205 of document complete\n",
      "0.823206585653 of document complete\n",
      "0.862406899255 of document complete\n",
      "0.901607212858 of document complete\n",
      "0.94080752646 of document complete\n",
      "0.980007840063 of document complete\n"
     ]
    }
   ],
   "source": [
    "stanford_text = []\n",
    "stanford_tag = []\n",
    "audible_text = []\n",
    "audible_tag = []\n",
    "for f in files:\n",
    "    print f\n",
    "    xml_data = open(f).read()\n",
    "    soup = BeautifulSoup(xml_data, 'lxml')\n",
    "    sentences = soup.find_all('s')\n",
    "    for j, s in enumerate(sentences):\n",
    "        if j % 100 == 0:\n",
    "            print '%s of document complete' % (j/len(sentences))\n",
    "        words = s.find_all(['w', 'c'])\n",
    "        doc = ' '.join(w.text.encode('utf8') for w in words)\n",
    "        try:\n",
    "            stanford_words = st.tag([doc])\n",
    "        except:\n",
    "            stanford_words = st.tag([doc.decode('ascii', 'ignore')])\n",
    "        if len(stanford_words) == len(words):\n",
    "            for i, w in enumerate(words):\n",
    "                if 'ner' in w.attrs:\n",
    "                    stanford_text.append(stanford_words[i][0])\n",
    "                    stanford_tag.append(stanford_words[i][1])\n",
    "                    audible_text.append(w.text)\n",
    "                    audible_tag.append(w.attrs['ner'])\n",
    "                else:\n",
    "                    stanford_text.append(stanford_words[i][0])\n",
    "                    stanford_tag.append(stanford_words[i][1])\n",
    "                    audible_text.append(w.text)\n",
    "                    audible_tag.append('O-')\n",
    "        else:\n",
    "            stanford_text, stanford_tag, audible_text, audible_tag = condense_stanford_tokens(s, stanford_text, stanford_tag, audible_text, audible_tag, st)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_corenlp = pd.DataFrame({'stanford_text':stanford_text,\n",
    "             'stanford_tag':stanford_tag,\n",
    "             'audible_text':audible_text,\n",
    "             'audible_tag':audible_tag})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need to standardize the tags to make the suitable for comparing with each other and for running through the conll scoring script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "O-     315619\n",
       "B-P      7735\n",
       "B-L      1126\n",
       "I-P       920\n",
       "B-O       373\n",
       "I-L       317\n",
       "I-O       226\n",
       "Name: cleaned_stanford_tags, dtype: int64"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from audible import clean_stanford_tags\n",
    "\n",
    "df_corenlp['cleaned_stanford_tags'] = df_corenlp.stanford_tag.apply(clean_stanford_tags)\n",
    "\n",
    "df_corenlp['prev_tag'] = df_corenlp.cleaned_stanford_tags.shift(1)\n",
    "\n",
    "df_corenlp['iob'] = 'O'\n",
    "df_corenlp['iob'][(df_corenlp.cleaned_stanford_tags!='O-') & \n",
    "                  (df_corenlp.cleaned_stanford_tags != df_corenlp.prev_tag)] = 'B'\n",
    "df_corenlp['iob'][(df_corenlp.cleaned_stanford_tags!='O-') & \n",
    "                 (df_corenlp.cleaned_stanford_tags == df_corenlp.prev_tag)] = 'I'\n",
    "\n",
    "\n",
    "df_corenlp['cleaned_stanford_tags'][df_corenlp.cleaned_stanford_tags!='O-'] = df_corenlp['iob']+df_corenlp['cleaned_stanford_tags']\n",
    "df_corenlp.cleaned_stanford_tags.value_counts()\n",
    "\n",
    "#df_corenlp.cleaned_stanford_tags.value_counts()\n",
    "#df[df.cleaned_spacy_tags=='O-P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "O-     315461\n",
       "I-P      5857\n",
       "B-P      3200\n",
       "B-L       743\n",
       "I-L       664\n",
       "I-O       235\n",
       "B-O       156\n",
       "Name: aud_clean, dtype: int64"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_corenlp['aud_clean'] = df_corenlp.audible_tag.apply(clean_audible_tags)\n",
    "df_corenlp['counter'] = df_corenlp.groupby('audible_tag').cumcount()\n",
    "\n",
    "df_corenlp['iob_sys'] = 'O'\n",
    "\n",
    "df_corenlp['iob_sys'][df_corenlp.counter==0] = 'B'\n",
    "df_corenlp['iob_sys'][df_corenlp.counter>0] = 'I'\n",
    "df_corenlp['aud_clean'][df_corenlp.aud_clean!='O-'] = df_corenlp['iob_sys']+df_corenlp['aud_clean']\n",
    "df_corenlp.aud_clean.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "outdat = df_corenlp[['audible_text', 'aud_clean', 'cleaned_stanford_tags']]\n",
    "outdat['audible_text'] = outdat.audible_text.apply(lambda x: x.replace(' ', ''))\n",
    "outdat.to_csv('temp_stanford', sep=' ', encoding='utf-8', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`processed 326316 tokens with 9108 phrases; found: 9234 phrases; correct: 7674.`\n",
    "\n",
    "`accuracy:  97.71%; precision:  83.11%; recall:  84.26%; FB1:  83.68\n",
    "                L: precision:  68.21%; recall:  73.70%; FB1:  70.85  1126\n",
    "                O: precision:  19.30%; recall:  34.62%; FB1:  24.78  373\n",
    "                P: precision:  88.35%; recall:  86.97%; FB1:  87.65  7735`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Above, we see that, just as Dan had suspected, the CoreNLP system is a closer approximation to whatever Kindle is using than is the Spacy system. Also interesting is that the performance across the three classes shows the same pattern as the Spacy system. That is, there's relatively good agreement for Persons, and relatively poor agreement on Organizations, with Locations placed in the middle.\n",
    "\n",
    "For next time, the clearest course of action is perform some error analysis. I'll examine places where these systems disagree to see if there's some obvious reason for these disparities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
