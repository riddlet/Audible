{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week's task is to try to run some taggers on the Audible data given to us by Dan. We have two sets of files. The first is an unlabeled dataset that contains approximately 25% of the text from 1000 books. The second dataset consists of 10 books from the first dataset in XML files that are labeled with parts of speech, word chunks (i.e. verb phrases, noun phrases), lemmatized, and the basic entities (People, Organizations, and Locations). Let's do some exploration of these files.\n",
    "\n",
    "First, let's find out how many individual words are given entity tags, and what type of tags they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lxml.etree as etree\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "\n",
    "files = glob.glob('data/audible/processedText/*.xml')\n",
    "entities = {'P': 0, 'L':0, 'O':0, 'D':0}\n",
    "total_tokens = 0\n",
    "for f in files:\n",
    "    xml_data = open(f).read()\n",
    "    soup = BeautifulSoup(xml_data, 'lxml')\n",
    "    for i in soup.find_all('w'):\n",
    "        for item in i:\n",
    "            total_tokens += 1\n",
    "            if 'ner' in i.attrs:\n",
    "                entities[i.attrs['ner'][0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9057 instances of P\n",
      "There are 1407 instances of L\n",
      "There are 391 instances of O\n",
      "There are 0 instances of D\n",
      "***** 272320 tokens ******\n"
     ]
    }
   ],
   "source": [
    "for i in entities.keys():\n",
    "    print('There are ' + str(entities[i]) + ' instances of ' + str(i))\n",
    "print('***** ' + str(total_tokens) + ' tokens ******')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the labeled data, we see that there are just over 270k individual tokens, and nearly 11k of them are labeled as entities. The majority of these entities (~9k) are people, with a few location tags, and a small number of Organizations. Dates are not labeled in these data, so we'll exclude them from our experimentation.\n",
    "\n",
    "One thing to note is that this method of counting double-counts some instances. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = glob.glob('data/audible/processedText/*.xml')\n",
    "total_tokens = 0\n",
    "unique_entities = {}\n",
    "for f in files:\n",
    "    xml_data = open(f).read()\n",
    "    soup = BeautifulSoup(xml_data, 'lxml')\n",
    "    for i in soup.find_all(lambda tag: 'ner' in tag.attrs):\n",
    "        if i['ner']+'_'+f[-14:-4] in unique_entities.keys():\n",
    "            unique_entities[i['ner']+'_'+f[-14:-4]] += 1\n",
    "        else:\n",
    "            unique_entities[i['ner']+'_'+f[-14:-4]] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entity that takes up the largest number of strings is:\n",
      "Harmonic Field of Glass Bells and Green Gig\n",
      "\n",
      "\n",
      "*****************\n",
      "The entire sentence that contains this entity is:\n",
      "We might add here that later on the constructors had an article published in a prominent scientific journal under the title of ' Recursive β – Metafunctions in the Special Case of a Bogus Polypolice Transmogrification Conversion on an Oscillating Harmonic Field of Glass Bells and Green Gig , Kerosene Lamp on the Left to Divert Attention , Solved by Beastly Incarceration – Concatenation ' , which was subsequently exploited by the tabloids as ' The Police State Rears Its Ugly Head ' .\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "longest_entity = max(unique_entities.iteritems(), key=operator.itemgetter(1))\n",
    "ent_id, file_id = longest_entity[0].split('_')\n",
    "xml_data = open('data/audible/processedText/'+file_id+'.xml').read()\n",
    "soup = BeautifulSoup(xml_data, 'lxml')\n",
    "entity = soup.findAll(ner=ent_id)\n",
    "print('The entity that takes up the largest number of strings is:')\n",
    "print(' '.join(i.text for i in entity))\n",
    "print('\\n')\n",
    "print('*****************')\n",
    "print('The entire sentence that contains this entity is:')\n",
    "print(' '.join(entity[0].parent.parent.text.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the former counting method, this entity would have been counted as 8 distinct entities, whereas it's actually meant to be one entity. Interestingly, this example is incorrectly labeled. This entity is labeled as an organization, but it's actually part of a title (and would make a great band name!). Regardless, this highlights the importance of why using a standard scoring approach wont work so well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Entities\n",
    "\n",
    "Below, I explore how Spacy handles entities in this text. The standard way of scoring this type of data is the one used for the CONLL shared tasks. The perl CONLL scoring script is available [here](http://www.cnts.ua.ac.be/conll2002/ner/bin/conlleval.txt), and I use this as my scorer. One of the difficulties here is that the spacy tokenizer works a bit differently than the tokenizer used on the Audible data. Below, I print out a sample of one of the documents with the ner tags. The printout is in the following format:\n",
    "\n",
    "\n",
    "|spacy_text   |spacy_tag   |******   |audible_text   |audible_tag   |\n",
    "|---|---|--:|---|---|\n",
    "|disemboweled   |O-   |******   |disemboweled   |O   |\n",
    "|,   |O-   |******   |,   |O   |\n",
    "|buried   |O-   |******   |buried   |O   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disemboweled\tO-\t******  disemboweled\tO\n",
      ",\tO-\t******  ,\tO\n",
      "buried\tO-\t******  buried\tO\n",
      "alive\tO-\t******  alive\tO\n",
      ",\tO-\t******  ,\tO\n",
      "crucified\tO-\t******  crucified\tO\n",
      "and\tO-\t******  and\tO\n",
      "burnt\tO-\t******  burnt\tO\n",
      "at\tO-\t******  at\tO\n",
      "the\tO-\t******  the\tO\n",
      "stake\tO-\t******  stake\tO\n",
      ",\tO-\t******  ,\tO\n",
      "after\tO-\t******  after\tO\n",
      "which\tO-\t******  which\tO\n",
      "your\tO-\t******  your\tO\n",
      "ashes\tO-\t******  ashes\tO\n",
      "shall\tO-\t******  shall\tO\n",
      "be\tO-\t******  be\tO\n",
      "sent\tO-\t******  sent\tO\n",
      "into\tO-\t******  into\tO\n",
      "orbit\tO-\t******  orbit\tO\n",
      "as\tO-\t******  as\tO\n",
      "a\tO-\t******  a\tO\n",
      "warning\tO-\t******  warning\tO\n",
      "and\tO-\t******  and\tO\n",
      "perpetual\tO-\t******  perpetual\tO\n",
      "reminder\tO-\t******  reminder\tO\n",
      "to\tO-\t******  to\tO\n",
      "all\tO-\t******  all\tO\n",
      "would\tO-\t******  would-be\tO\n",
      "-\tO-\t******  regicides\tO\n",
      "be\tO-\t******  ,\tO\n",
      "regicides\tO-\t******  amen\tO\n",
      ",\tO-\t******  .\tO\n",
      "amen\tO-\t******  '\tO\n",
      ".\tO-\t******  '\tO\n",
      "'\tO-\t******  Ca\tO\n",
      "'\tO-\t******  n't\tO\n",
      "Ca\tO-\t******  you\tO\n",
      "n't\tO-\t******  wait\tO\n",
      "you\tO-\t******  a\tO\n",
      "wait\tO-\t******  bit\tO\n",
      "a\tO-\t******  ?\tO\n",
      "bit\tO-\t******  '\tO\n",
      "?\tO-\t******  asked\tO\n",
      "'\tO-\t******  Trurl\tP485\n",
      "asked\tO-\t******  .\tO\n",
      "Trurl\tB-PERSON\t******  '\tO\n",
      ".\tO-\t******  You\tO\n",
      "'\tO-\t******  see\tO\n"
     ]
    }
   ],
   "source": [
    "words = soup.find_all(['w', 'c'])\n",
    "doc = nlp(' '.join(w.text for w in words))\n",
    "start = 175\n",
    "subdoc = doc[start:225]\n",
    "for i, w in enumerate(subdoc):\n",
    "    if 'ner' in words[i+start].attrs:\n",
    "        print w.text + '\\t' + w.ent_iob_ + '-' + w.ent_type_ + '\\t******  ' + words[i+start].text + '\\t' + words[i+start].attrs['ner']\n",
    "    else:\n",
    "        print w.text + '\\t' + w.ent_iob_ + '-' + w.ent_type_ + '\\t******  ' + words[i+start].text + '\\t' + 'O'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that the compound word would-be (about halfway down) is treated differently by the two systems. This makes lining up the annotations a bit of a challenge. Especially since it may mean that Spacy will provide multiple tags to the same word which is given only one tag from Audible. To counteract this, I'll group together the words that are split apart, and give them an entity tag if any of the tokens contains one. Fortunately, there were no cases where these split words contained two different tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ugh, this is ugly.\n",
    "def condense_tokens(spacy_doc, audible_words, position, offset):\n",
    "    spacy_tokens = nlp(audible_words[position+offset[1]].text)\n",
    "    if 'ner' in audible_words[i+offset[1]].attrs:\n",
    "        spacy_text = ''.join(j.text for j in spacy_doc[i+offset[0]:i+offset[0]+len(spacy_tokens)]) \n",
    "        spacy_tag = ''.join(j.ent_iob_ for j in spacy_doc[i+offset[0]:i+offset[0]+len(spacy_tokens)]) + '-' + \\\n",
    "        ''.join(j.ent_type_ for j in spacy_doc[i+offset[0]:i+offset[0]+len(spacy_tokens)]) \n",
    "        audible_text = audible_words[i+offset[1]].text \n",
    "        audible_tag = audible_words[i+offset[1]].attrs['ner']\n",
    "    else:\n",
    "        spacy_text = ''.join(j.text for j in spacy_doc[i+offset[0]:i+offset[0]+len(spacy_tokens)])\n",
    "        spacy_tag = ''.join(j.ent_iob_ for j in spacy_doc[i+offset[0]:i+offset[0]+len(spacy_tokens)]) + '-' + \\\n",
    "        ''.join(j.ent_type_ for j in spacy_doc[i+offset[0]:i+offset[0]+len(spacy_tokens)])\n",
    "        audible_text = audible_words[i+offset[1]].text \n",
    "        audible_tag = 'O-'\n",
    "    \n",
    "    offset[0] += len(spacy_tokens)-1\n",
    "    return(spacy_text, spacy_tag, audible_text, audible_tag, offset)\n",
    "\n",
    "spacy_text = []\n",
    "spacy_tag = []\n",
    "audible_text = []\n",
    "audible_tag = []\n",
    "for f in files:\n",
    "    xml_data = open(f).read()\n",
    "    soup = BeautifulSoup(xml_data, 'lxml')\n",
    "    words = soup.find_all(['w', 'c'])\n",
    "    doc = nlp(' '.join(w.text for w in words))\n",
    "    offset = [0, 0] #spacy, audible\n",
    "    subdoc = doc #for testing\n",
    "    for i, w in enumerate(subdoc):\n",
    "        if i+offset[1] == len(words):\n",
    "            break\n",
    "        spacy_tokens = nlp(words[i+offset[1]].text) #check to see if the tokenizations match\n",
    "\n",
    "        if len(spacy_tokens)==1: #if they match\n",
    "            if 'ner' in words[i+offset[1]].attrs:\n",
    "                spacy_text.append(subdoc[i+offset[0]].text)\n",
    "                spacy_tag.append(subdoc[i+offset[0]].ent_iob_ + '-' + subdoc[i+offset[0]].ent_type_)\n",
    "                audible_text.append(words[i+offset[1]].text)\n",
    "                audible_tag.append(words[i+offset[1]].attrs['ner'])\n",
    "            else:\n",
    "                spacy_text.append(subdoc[i+offset[0]].text)\n",
    "                spacy_tag.append(subdoc[i+offset[0]].ent_iob_ + '-' + subdoc[i+offset[0]].ent_type_)\n",
    "                audible_text.append(words[i+offset[1]].text)\n",
    "                audible_tag.append('O')\n",
    "\n",
    "        if len(spacy_tokens)>1: #if spacy splits it up into more tokens\n",
    "            stext, stag, atext, atag, offset = condense_tokens(doc, words, i, offset)\n",
    "            spacy_text.append(stext)\n",
    "            spacy_tag.append(stag)\n",
    "            audible_text.append(atext)\n",
    "            audible_tag.append(atag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'spacy_text':spacy_text,\n",
    "             'spacy_tag':spacy_tag,\n",
    "             'audible_text':audible_text,\n",
    "             'audible_tag':audible_tag})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing Tags\n",
    "Spacy's entity recognizer includes many other categories that are not listed in the Audible data. The next bits of code removes those entities from the tags provided by Spacy and then makes the remaining ones in a standard format that is similar to that expected by the CONLL script. I then standardize the tags from the Audible data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O-     315119\n",
       "B-P      7195\n",
       "B-L      1341\n",
       "I-P       977\n",
       "B-O       904\n",
       "I-O       592\n",
       "I-L       188\n",
       "Name: cleaned_spacy_tags, dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def clean_spacy_tags(tags):\n",
    "    prog = re.compile('^(B|I|O)+-(CARDINAL|PERCENT|DATE|NORP|TIME|QUANTITY|MONEY|ORDINAL|LAW|FAC|WORK_OF_ART|PRODUCT|EVENT|LANGUAGE)+$')\n",
    "    if prog.match(tags):\n",
    "        out = 'O-'\n",
    "    else:\n",
    "        out = tags\n",
    "    prog = re.compile('^O+-$')\n",
    "    if prog.match(out):\n",
    "        out = 'O-'\n",
    "    prog = re.compile('^(B|I|O)(B|I|O)*?-GPE$')\n",
    "    if prog.match(out):\n",
    "        matched = prog.match(out).group(1)\n",
    "        out = matched+'-L'\n",
    "    prog = re.compile('^((B|I)|O)+-(PERSON|ORG|LOC)+$')\n",
    "    if prog.match(out):\n",
    "        matched1 = prog.match(out).group(2)\n",
    "        matched2 = prog.match(out).group(3)[0]\n",
    "        out = matched1+'-'+matched2[0]\n",
    "    return(out)\n",
    "\n",
    "df['cleaned_spacy_tags'] = df.spacy_tag.apply(clean_spacy_tags)\n",
    "\n",
    "df.cleaned_spacy_tags.value_counts()\n",
    "#df[df.cleaned_spacy_tags=='O-P']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "O-     315461\n",
       "I-P      5857\n",
       "B-P      3200\n",
       "B-L       743\n",
       "I-L       664\n",
       "I-O       235\n",
       "B-O       156\n",
       "Name: aud_clean, dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_audible_tags(item):\n",
    "    prog = re.compile('^O$')\n",
    "    if prog.match(item):\n",
    "        out = 'O-'\n",
    "    else:\n",
    "        out = item\n",
    "    prog = re.compile('^(P|L|O)([0-9])+$')\n",
    "    if prog.match(out):\n",
    "        matched = '-'+prog.match(out).group(1)\n",
    "        out = matched\n",
    "    return(out)\n",
    "\n",
    "df['aud_clean'] = df.audible_tag.apply(clean_audible_tags)\n",
    "df['counter'] = df.groupby('audible_tag').cumcount()\n",
    "\n",
    "df['iob_sys'] = 'O'\n",
    "\n",
    "df['iob_sys'][df.counter==0] = 'B'\n",
    "df['iob_sys'][df.counter>0] = 'I'\n",
    "df['aud_clean'][df.aud_clean!='O-'] = df['iob_sys']+df['aud_clean']\n",
    "df.aud_clean.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy Scoring\n",
    "Below, I write a file for the conll script. I think that evaluation here consists of entire chunks, rather than individualized tokens. As such, if System one identified \"Susan Brown\" as one entity chunk, and system two only identified \"Susan\" as the chunk, then this would be incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "outdat = df[['audible_text', 'aud_clean', 'cleaned_spacy_tags']]\n",
    "outdat['audible_text'] = outdat.audible_text.apply(lambda x: x.replace(' ', ''))\n",
    "outdat.to_csv('temp', sep=' ', encoding='utf-8', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processed 326316 tokens with 9108 phrases; found: 9488 phrases; correct: 6682.\n",
    "\n",
    "`accuracy:  97.33%; precision:  70.43%; recall:  73.36%; FB1:  71.86`\n",
    "\n",
    "                L: precision:  48.88%; recall:  62.96%; FB1:  55.03  1342\n",
    "                \n",
    "                O: precision:   5.53%; recall:  25.00%; FB1:   9.06  940\n",
    "                \n",
    "                P: precision:  82.90%; recall:  76.02%; FB1:  79.31  7206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
