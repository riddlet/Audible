{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of this week's work, I'm going to apply the pre-trained model described in the [Lample et al](https://arxiv.org/abs/1603.01360) paper. We've been referring to this as a \"Glample\" model, so I'll stick with that. In this work, they use a bi-directional LSTM with a conditional random field on top to output the classes. Performance is roughly on par with other systems, but provides the advantage of being language agnostic and does not rely on any external labeled data (e.g. gazetteer). Performance on Spanish and German surpasses previous state-of-the-art.\n",
    "\n",
    "To evaluate the glample model, I need to write an output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxml.etree as etree\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "files = glob.glob('../data/audible/processedText/*.xml')\n",
    "outdat = open('audible_files.txt', 'w')\n",
    "for f in files:\n",
    "    xml_data = open(f).read()\n",
    "    soup = BeautifulSoup(xml_data, 'lxml')\n",
    "    sentences = soup.find_all('s')\n",
    "    for s in sentences:\n",
    "        tokens = s.find_all(['w', 'c'])\n",
    "        for i in tokens:\n",
    "            outdat.write(i.text.encode('utf8'))\n",
    "            outdat.write(' ')\n",
    "        outdat.write('\\n')\n",
    "\n",
    "outdat.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xml_data = open(files[0]).read()\n",
    "soup = BeautifulSoup(xml_data, 'lxml')\n",
    "sentences = soup.find_all('s')\n",
    "sentences = sentences[125:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outdat = open('audible_files.txt', 'w')\n",
    "for s in sentences:\n",
    "    tokens = s.find_all(['w', 'c'])\n",
    "    for i in tokens:\n",
    "        outdat.write(i.text.encode('utf8'))\n",
    "        outdat.write(' ')\n",
    "    outdat.write('\\n')\n",
    "outdat.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"', 'Is', 'this', 'Mr.', 'David', 'Robicheaux', '?', '\"']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.text.encode('utf8') for i in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from model import Model\n",
    "from loader import prepare_sentence\n",
    "model = Model(model_path='models/english/')\n",
    "parameters = model.parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load reverse mappings\n",
    "word_to_id, char_to_id, tag_to_id = [\n",
    "    {v: k for k, v in x.items()}\n",
    "    for x in [model.id_to_word, model.id_to_char, model.id_to_tag]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'str_words': [u'\"', u'Do', u'you', u'like', u'trouble', u'?', u'\"'], 'chars': [[43], [41, 5], [19, 5, 13], [9, 4, 27, 0], [2, 6, 5, 13, 21, 9, 0], [69], [43]], 'words': [13, 12197, 277, 504, 3380, 1706, 13], 'caps': [0, 2, 0, 0, 0, 0, 0]}\n",
      "{'str_words': [u'she', u'asked', u'.'], 'chars': [[7, 11, 0], [1, 7, 27, 0, 10], [18]], 'words': [163, 499, 1], 'caps': [0, 0, 0]}\n",
      "{'str_words': [u'\"', u'I', u'do', u\"n't\", u'seek', u'it', u'out', u',', u'\"', u'I', u'said', u'.'], 'chars': [[43], [30], [10, 5], [3, 45, 2], [7, 0, 0, 27], [4, 2], [5, 13, 2], [23], [43], [30], [7, 1, 4, 10], [18]], 'words': [13, 62, 165, 190, 1388, 37, 67, 2, 13, 62, 15, 1], 'caps': [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]}\n",
      "{'str_words': [u'\"', u'I', u'heard', u'you', u'were', u'a', u'Twelve-Step', u'person', u'.', u'\"'], 'chars': [[43], [30], [11, 0, 1, 6, 10], [19, 5, 13], [20, 0, 6, 0], [1], [28, 20, 0, 9, 24, 0, 22, 25, 2, 0, 16], [16, 0, 6, 7, 5, 3], [18], [43]], 'words': [13, 62, 2093, 277, 40, 9, 0, 2920, 1, 13], 'caps': [0, 1, 0, 0, 0, 0, 2, 0, 0, 0]}\n",
      "{'str_words': [u'\"', u'I', u\"'m\", u'in', u'AA', u',', u'if', u'that', u\"'s\", u'what', u'you', u'mean', u'.', u'\"'], 'chars': [[43], [30], [45, 14], [4, 3], [26, 26], [23], [4, 15], [2, 11, 1, 2], [45, 7], [20, 11, 1, 2], [19, 5, 13], [14, 0, 1, 3], [18], [43]], 'words': [13, 62, 857, 7, 11056, 2, 189, 27, 16, 334, 277, 3297, 1, 13], 'caps': [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'str_words': [u'\"', u'Maybe', u'that', u\"'s\", u'what', u'you', u'need', u'to', u'keep', u'doing', u'and', u'not', u'complicate', u'things', u'.', u'\"'], 'chars': [[43], [34, 1, 19, 21, 0], [2, 11, 1, 2], [45, 7], [20, 11, 1, 2], [19, 5, 13], [3, 0, 0, 10], [2, 5], [27, 0, 0, 16], [10, 5, 4, 3, 17], [1, 3, 10], [3, 5, 2], [12, 5, 14, 16, 9, 4, 12, 1, 2, 0], [2, 11, 4, 3, 17, 7], [18], [43]], 'words': [13, 3566, 27, 16, 334, 277, 982, 8, 845, 1268, 12, 39, 191725, 1481, 1, 13], 'caps': [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'str_words': [u'\"', u'I', u\"'d\", u'sure', u'like', u'to', u'have', u'lunch', u'with', u'you', u'.', u'\"'], 'chars': [[43], [30], [45, 10], [7, 13, 6, 0], [9, 4, 27, 0], [2, 5], [11, 1, 24, 0], [9, 13, 3, 12, 11], [20, 4, 2, 11], [19, 5, 13], [18], [43]], 'words': [13, 62, 5848, 4640, 504, 8, 43, 2893, 25, 277, 1, 13], 'caps': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'str_words': [u'She', u'looked', u'out', u'the', u'window', u'at', u'Alice', u'Plantation', u',', u'the', u'acres', u'of', u'clipped', u'St.', u'Augustine', u'grass', u'and', u'the', u'flowers', u'growing', u'along', u'the', u'brick', u'base', u'of', u'the', u'building', u'.'], 'chars': [[25, 11, 0], [9, 5, 5, 27, 0, 10], [5, 13, 2], [2, 11, 0], [20, 4, 3, 10, 5, 20], [1, 2], [26, 9, 4, 12, 0], [40, 9, 1, 3, 2, 1, 2, 4, 5, 3], [23], [2, 11, 0], [1, 12, 6, 0, 7], [5, 15], [12, 9, 4, 16, 16, 0, 10], [25, 2, 18], [26, 13, 17, 13, 7, 2, 4, 3, 0], [17, 6, 1, 7, 7], [1, 3, 10], [2, 11, 0], [15, 9, 5, 20, 0, 6, 7], [17, 6, 5, 20, 4, 3, 17], [1, 9, 5, 3, 17], [2, 11, 0], [21, 6, 4, 12, 27], [21, 1, 7, 0], [5, 15], [2, 11, 0], [21, 13, 4, 9, 10, 4, 3, 17], [18]], 'words': [689, 1375, 67, 3, 4675, 23, 59864, 142272, 2, 3, 4313, 5, 190661, 1886, 63631, 205192, 12, 3, 202508, 2307, 1350, 3, 16155, 2494, 5, 3, 1448, 1], 'caps': [2, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'str_words': [u'\"', u'Can', u'we', u'invite', u'another', u'person', u'to', u'join', u'us', u',', u'an', u'elderly', u'lady', u'who', u'volunteers', u'at', u'the', u'agency', u'?', u'\"'], 'chars': [[43], [29, 1, 3], [20, 0], [4, 3, 24, 4, 2, 0], [1, 3, 5, 2, 11, 0, 6], [16, 0, 6, 7, 5, 3], [2, 5], [52, 5, 4, 3], [13, 7], [23], [1, 3], [0, 9, 10, 0, 6, 9, 19], [9, 1, 10, 19], [20, 11, 5], [24, 5, 9, 13, 3, 2, 0, 0, 6, 7], [1, 2], [2, 11, 0], [1, 17, 0, 3, 12, 19], [69], [43]], 'words': [13, 74476, 103, 210368, 402, 2920, 8, 1664, 620, 2, 42, 5442, 5557, 45, 10866, 23, 3, 395, 1706, 13], 'caps': [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'str_words': [u'she', u'asked', u'.'], 'chars': [[7, 11, 0], [1, 7, 27, 0, 10], [18]], 'words': [163, 499, 1], 'caps': [0, 0, 0]}\n",
      "{'str_words': [u'\"', u'That', u\"'d\", u'be', u'fine', u',', u'\"', u'I', u'said', u'.'], 'chars': [[43], [28, 11, 1, 2], [45, 10], [21, 0], [15, 4, 3, 0], [23], [43], [30], [7, 1, 4, 10], [18]], 'words': [13, 717, 5848, 41, 2539, 2, 13, 62, 15, 1], 'caps': [0, 2, 0, 0, 0, 0, 0, 1, 0, 0]}\n",
      "{'str_words': [u'I', u'could', u'feel', u'her', u'eyes', u'on', u'the', u'side', u'of', u'my', u'face', u'.'], 'chars': [[30], [12, 5, 13, 9, 10], [15, 0, 0, 9], [11, 0, 6], [0, 19, 0, 7], [5, 3], [2, 11, 0], [7, 4, 10, 0], [5, 15], [14, 19], [15, 1, 12, 0], [18]], 'words': [62, 118, 1459, 101, 7039, 14, 3, 534, 5, 417, 698, 1], 'caps': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'str_words': [u'Up', u'ahead', u',', u'a', u'black', u'cloud', u'moved', u'across', u'the', u'sun', u',', u'dropping', u'the', u'countryside', u'into', u'shadow', u'.'], 'chars': [[48, 16], [1, 11, 0, 1, 10], [23], [1], [21, 9, 1, 12, 27], [12, 9, 5, 13, 10], [14, 5, 24, 0, 10], [1, 12, 6, 5, 7, 7], [2, 11, 0], [7, 13, 3], [23], [10, 6, 5, 16, 16, 4, 3, 17], [2, 11, 0], [12, 5, 13, 3, 2, 6, 19, 7, 4, 10, 0], [4, 3, 2, 5], [7, 11, 1, 10, 5, 20], [18]], 'words': [4291, 454, 2, 9, 1062, 190748, 1944, 630, 3, 236609, 2, 9824, 3, 9724, 106, 19183, 1], 'caps': [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'str_words': [u'\"', u'Do', u'you', u'have', u'any', u'idea', u'who', u'the', u'man', u'in', u'the', u'boat', u'might', u'have', u'been', u'?', u'\"'], 'chars': [[43], [41, 5], [19, 5, 13], [11, 1, 24, 0], [1, 3, 19], [4, 10, 0, 1], [20, 11, 5], [2, 11, 0], [14, 1, 3], [4, 3], [2, 11, 0], [21, 5, 1, 2], [14, 4, 17, 11, 2], [11, 1, 24, 0], [21, 0, 0, 3], [69], [43]], 'words': [13, 12197, 277, 43, 202, 2881, 45, 3, 322, 7, 3, 1643, 597, 43, 52, 1706, 13], 'caps': [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'str_words': [u'she', u'said', u'.'], 'chars': [[7, 11, 0], [7, 1, 4, 10], [18]], 'words': [163, 15, 1], 'caps': [0, 0, 0]}\n",
      "{'str_words': [u'\"', u'Probably', u'just', u'a', u'guy', u'shooting', u'water', u'moccasins', u',', u'\"', u'I', u'said', u'.'], 'chars': [[43], [40, 6, 5, 21, 1, 21, 9, 19], [52, 13, 7, 2], [1], [17, 13, 19], [7, 11, 5, 5, 2, 4, 3, 17], [20, 1, 2, 0, 6], [14, 5, 12, 12, 1, 7, 4, 3, 7], [23], [43], [30], [7, 1, 4, 10], [18]], 'words': [13, 14407, 171, 9, 17411, 1578, 1485, 0, 2, 13, 62, 15, 1], 'caps': [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]}\n",
      "{'str_words': [u'\"', u'That', u'seems', u'kind', u'of', u'cavalier', u',', u'\"', u'she', u'said', u'.'], 'chars': [[43], [28, 11, 1, 2], [7, 0, 0, 14, 7], [27, 4, 3, 10], [5, 15], [12, 1, 24, 1, 9, 4, 0, 6], [23], [43], [7, 11, 0], [7, 1, 4, 10], [18]], 'words': [13, 717, 4602, 5555, 5, 189470, 2, 13, 163, 15, 1], 'caps': [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'str_words': [u'\"', u'When', u'the', u'pros', u'punch', u'your', u'ticket', u',', u'they', u\"'re\", u'at', u'your', u'throat', u'before', u'you', u'know', u'it', u'.'], 'chars': [[43], [47, 11, 0, 3], [2, 11, 0], [16, 6, 5, 7], [16, 13, 3, 12, 11], [19, 5, 13, 6], [2, 4, 12, 27, 0, 2], [23], [2, 11, 0, 19], [45, 6, 0], [1, 2], [19, 5, 13, 6], [2, 11, 6, 5, 1, 2], [21, 0, 15, 5, 6, 0], [19, 5, 13], [27, 3, 5, 20], [4, 2], [18]], 'words': [13, 1017, 3, 18687, 18725, 1399, 7510, 2, 64, 583, 23, 1399, 19696, 112, 277, 846, 37, 1], 'caps': [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'str_words': [u'The', u'guy', u'in', u'the', u'boat', u'was', u'just', u'a', u'guy', u'in', u'a', u'boat', u',', u'\"', u'I', u'said', u'.'], 'chars': [[28, 11, 0], [17, 13, 19], [4, 3], [2, 11, 0], [21, 5, 1, 2], [20, 1, 7], [52, 13, 7, 2], [1], [17, 13, 19], [4, 3], [1], [21, 5, 1, 2], [23], [43], [30], [7, 1, 4, 10], [18]], 'words': [19, 17411, 7, 3, 1643, 20, 171, 9, 17411, 7, 9, 1643, 2, 13, 62, 15, 1], 'caps': [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]}\n",
      "{'str_words': [u'\"', u'I', u'worked', u'at', u'a', u'mission', u'in', u'Guatemala', u'during', u'the', u'civil', u'war', u'.'], 'chars': [[43], [30], [20, 5, 6, 27, 0, 10], [1, 2], [1], [14, 4, 7, 7, 4, 5, 3], [4, 3], [42, 13, 1, 2, 0, 14, 1, 9, 1], [10, 13, 6, 4, 3, 17], [2, 11, 0], [12, 4, 24, 4, 9], [20, 1, 6], [18]], 'words': [13, 62, 2154, 23, 9, 1137, 7, 98196, 239, 3, 877, 312, 1], 'caps': [0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]}\n",
      "{'str_words': [u'Men', u'with', u'binoculars', u'and', u'guns', u'did', u\"n't\", u'use', u'them', u'to', u'hunt', u'snakes', u',', u'\"', u'she', u'replied', u'.'], 'chars': [[34, 0, 3], [20, 4, 2, 11], [21, 4, 3, 5, 12, 13, 9, 1, 6, 7], [1, 3, 10], [17, 13, 3, 7], [10, 4, 10], [3, 45, 2], [13, 7, 0], [2, 11, 0, 14], [2, 5], [11, 13, 3, 2], [7, 3, 1, 27, 0, 7], [23], [43], [7, 11, 0], [6, 0, 16, 9, 4, 0, 10], [18]], 'words': [344, 25, 186765, 12, 5510, 168, 190, 765, 180, 8, 3274, 233796, 2, 13, 163, 5685, 1], 'caps': [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'str_words': [u'CHAPTER'], 'chars': [[29, 44, 26, 40, 28, 31, 33]], 'words': [0], 'caps': [1]}\n",
      "{'str_words': [u'11'], 'chars': [[]], 'words': [27286], 'caps': [0]}\n",
      "{'str_words': [u'THE', u'PHONE', u'on', u'my', u'kitchen', u'counter', u'rang', u'early', u'Monday', u'morning', u'.'], 'chars': [[28, 44, 31], [40, 44, 35, 32, 31], [5, 3], [14, 19], [27, 4, 2, 12, 11, 0, 3], [12, 5, 13, 3, 2, 0, 6], [6, 1, 3, 17], [0, 1, 6, 9, 19], [34, 5, 3, 10, 1, 19], [14, 5, 6, 3, 4, 3, 17], [18]], 'words': [1119, 138323, 14, 417, 211408, 16547, 18783, 254, 95, 506, 1], 'caps': [1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0]}\n",
      "{'str_words': [u'\"', u'Is', u'this', u'Mr.', u'David', u'Robicheaux', u'?', u'\"'], 'chars': [[43], [30, 7], [2, 11, 4, 7], [34, 6, 18], [41, 1, 24, 4, 10], [33, 5, 21, 4, 12, 11, 0, 1, 13, 49], [69], [43]], 'words': [13, 13103, 69, 6416, 409, 0, 1706, 13], 'caps': [0, 2, 0, 2, 2, 2, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "with codecs.open('audible_files.txt', 'r', 'utf-8') as f_input:\n",
    "    count = 0\n",
    "    for line in f_input:\n",
    "        words = line.rstrip().split()\n",
    "        sentence = prepare_sentence(words, word_to_id, char_to_id, lower=parameters['lower'])\n",
    "        print sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import zero_digits\n",
    "zero_digits('11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But\n",
      "I\n",
      "was\n",
      "all\n",
      "out\n",
      "of\n",
      "Purple\n",
      "Hearts\n",
      "and\n",
      "had\n",
      "decided\n",
      "that\n",
      "Honoria\n",
      "was\n",
      "going\n",
      "to\n",
      "leave\n",
      "of\n",
      "her\n",
      "own\n",
      "accord\n",
      "or\n",
      "be\n",
      "picked\n",
      "up\n",
      "by\n",
      "a\n",
      "cruiser\n",
      ".\n",
      "My\n",
      "determination\n",
      "suddenly\n",
      "dissipated\n",
      "when\n",
      "I\n",
      "looked\n",
      "out\n",
      "the\n",
      "front\n",
      "window\n",
      "and\n",
      "saw\n",
      "the\n",
      "Chalonses\n",
      "'\n",
      "handyman\n",
      ",\n",
      "with\n",
      "his\n",
      "son\n",
      "and\n",
      "Sister\n",
      "Molly\n",
      "next\n",
      "to\n",
      "him\n",
      ",\n",
      "turn\n",
      "into\n",
      "my\n",
      "driveway\n",
      ".\n",
      "\"\n",
      "I\n",
      "'m\n",
      "going\n",
      "to\n",
      "talk\n",
      "to\n",
      "some\n",
      "people\n",
      "out\n",
      "front\n",
      ".\n",
      "There\n",
      "'s\n",
      "no\n",
      "need\n",
      "for\n",
      "you\n",
      "to\n",
      "leave\n",
      "right\n",
      "now\n",
      ",\n",
      "\"\n",
      "I\n",
      "said\n",
      "to\n",
      "Honoria\n",
      ".\n",
      "\"\n",
      "Too\n",
      "late\n",
      ",\n",
      "my\n",
      "love\n",
      ",\n",
      "\"\n",
      "she\n",
      "said\n",
      ".\n",
      "She\n",
      "walked\n",
      "out\n",
      "the\n",
      "front\n",
      "door\n",
      "and\n",
      "down\n",
      "the\n",
      "street\n",
      "toward\n",
      "the\n",
      "Shadows\n",
      ",\n",
      "her\n",
      "purse\n",
      "swinging\n",
      "from\n",
      "a\n",
      "shoulder\n",
      "string\n",
      ".\n",
      "I\n",
      "stood\n",
      "on\n",
      "the\n",
      "gallery\n",
      ",\n",
      "barefoot\n",
      ",\n",
      "unshaved\n",
      ",\n",
      "looking\n",
      "down\n",
      "at\n",
      "Molly\n",
      "Boyle\n",
      ",\n",
      "my\n",
      "face\n",
      "burning\n",
      ".\n",
      "\"\n",
      "I\n",
      "should\n",
      "have\n",
      "called\n",
      "first\n",
      ",\n",
      "I\n",
      "guess\n",
      ",\n",
      "but\n",
      "Tee\n",
      "Bleu\n",
      "says\n",
      "he\n",
      "knows\n",
      "where\n",
      "the\n",
      "boat\n",
      "is\n",
      ",\n",
      "\"\n",
      "she\n",
      "said\n",
      ",\n",
      "speaking\n",
      "awkwardly\n",
      "and\n",
      "too\n",
      "fast\n",
      ",\n",
      "trying\n",
      "to\n",
      "hide\n",
      "her\n",
      "embarrassment\n",
      "at\n",
      "my\n",
      "situation\n",
      ".\n",
      "\"\n",
      "Which\n",
      "boat\n",
      "?\n",
      "\"\n",
      "I\n",
      "said\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(opts.input, 'r', 'utf-8') as f_input:\n",
    "    count = 0\n",
    "    for line in f_input:\n",
    "        words = line.rstrip().split()\n",
    "        if line:\n",
    "            # Lowercase sentence\n",
    "            if parameters['lower']:\n",
    "                line = line.lower()\n",
    "            # Replace all digits with zeros\n",
    "            if parameters['zeros']:\n",
    "                line = zero_digits(line)\n",
    "            # Prepare input\n",
    "            sentence = prepare_sentence(words, word_to_id, char_to_id,\n",
    "                                        lower=parameters['lower'])\n",
    "            input = create_input(sentence, parameters, False)\n",
    "            # Decoding\n",
    "            if parameters['crf']:\n",
    "                y_preds = np.array(f_eval(*input))[1:-1]\n",
    "            else:\n",
    "                y_preds = f_eval(*input).argmax(axis=1)\n",
    "            y_preds = [model.id_to_tag[y_pred] for y_pred in y_preds]\n",
    "            # Output tags in the IOB2 format\n",
    "            if parameters['tag_scheme'] == 'iobes':\n",
    "                y_preds = iobes_iob(y_preds)\n",
    "            # Write tags\n",
    "            assert len(y_preds) == len(words)\n",
    "            f_output.write('%s\\n' % ' '.join('%s%s%s' % (w, opts.delimiter, y)\n",
    "                                             for w, y in zip(words, y_preds)))\n",
    "        else:\n",
    "            f_output.write('\\n')\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print count\n",
    "\n",
    "print '---- %i lines tagged in %.4fs ----' % (count, time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
